<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://ibopeng.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ibopeng.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-31T17:49:38+00:00</updated><id>https://ibopeng.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Position Embedding in LLMs</title><link href="https://ibopeng.github.io/blog/2026/position-embed/" rel="alternate" type="text/html" title="Position Embedding in LLMs"/><published>2026-01-15T00:00:00+00:00</published><updated>2026-01-15T00:00:00+00:00</updated><id>https://ibopeng.github.io/blog/2026/position-embed</id><content type="html" xml:base="https://ibopeng.github.io/blog/2026/position-embed/"><![CDATA[<h2 id="preliminary">Preliminary</h2> <p>If we look at the architecture of the original Transformer (the model that powers everything from GPT to Llama), we will find a peculiar design choice right at the beginning. Before the neural network does any heavy lifting, it adds a “Position Embedding” to the word embedding.</p> <p>Why is this necessary, and how does it work?</p> <h3 id="the-problem-the-bag-of-words-flaw">The Problem: The “Bag of Words” Flaw</h3> <p>To understand position embeddings, we first have to understand what a Transformer lacks compared to its predecessors.</p> <p>Previous architectures, like Recurrent Neural Networks (RNNs), processed words sequentially: they looked at word 1, then word 2, then word 3. The “position” was inherent in the order of processing.</p> <p>Transformers, however, process all tokens in a sequence without considering their positions in the sequence. Without some extra help, the model sees the sentence as a “bag of words.” To a raw Transformer, the sentence:</p> <p><em>“The dog bit the man”</em></p> <p>Looks mathematically identical to:</p> <p><em>“The man bit the dog”</em></p> <p>Because the model has no inherent sense of order, we must explicitly inject position information into the data. This is where <strong>Position Embeddings</strong> come in. They assign a unique vector to every index ($0, 1, 2, … T$) in the sequence.</p> <p>Let $S = {w_i}<em>{i=1}^N$ be a sequence of $N$ input tokens with $w_i$ being the $i^{th}$ token. Each $w_i$ is mapped to a $d</em>{\text{model}}$-dimensional embedding vector $\boldsymbol{x}<em>i \in \mathbb{R}^{d</em>\text{model}}$ without position information. These token embeddings $X = {\boldsymbol{x}<em>i}</em>{i=1}^N$, along with position information, are then transformed into queries, keys, and values used in the self-attention layer of the Transformer architecture.</p> \[\begin{aligned} \boldsymbol{q}_m &amp;= f_q(\boldsymbol{x}_m, m) \\ \boldsymbol{k}_n &amp;= f_k(\boldsymbol{x}_n, n) \\ \boldsymbol{v}_n &amp;= f_v(\boldsymbol{x}_n, n), \end{aligned} \tag{1}\] <p>where $\boldsymbol{q}_m, \boldsymbol{k}_n$ and $\boldsymbol{v}_n$ represent the $m^{\text{th}}$ and $n^{\text{th}}$ positions of query $Q \in \mathbb{R}^{N\times d_k}$, key $K \in \mathbb{R}^{N\times d_k}$, and value $V \in \mathbb{R}^{N\times d_v}$, respectively, as in the self-attention mechanism.</p> \[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\] <p>Note that the dimension change from $d_\text{model}$ to $d_k$ or $d_v$ because the Transformer doesn’t use $X$ directly for attention. It multiplies $X$ by three separate learnable weight matrices ($W^Q$, $W^K$, $W^V$) to project the data into the “head” dimension.</p> <h2 id="absolute-position-embedding">Absolute Position Embedding</h2> <h3 id="sinusoidal-embeddings">Sinusoidal Embeddings</h3> <p>This method was introduced in the original Transformer paper by Vaswani et al. <d-cite key="vaswani2017attention"></d-cite> Instead of learning the position vectors during training, the authors calculated them using fixed mathematical formulas based on sine and cosine waves.</p> <p><strong>The Intuition</strong></p> <p>Imagine a clock with many hands moving at different speeds. By looking at the positions of all the hands simultaneously, you can determine the exact time. Sinusoidal embeddings work similarly: each dimension of the position vector corresponds to a sinusoid of a different frequency.</p> <p><strong>The Math</strong></p> <p>For a specific position and a specific dimension , the embedding is calculated as:</p> \[\begin{aligned} PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_\text{model}}}\right) \\ PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_\text{model}}}\right) \end{aligned} \tag{2}\] <p>Where:</p> <ul> <li>$pos$ is the position of the token.</li> <li>$d_\text{model}$ is the size of the embedding vector.</li> <li>The wavelengths form a geometric progression from $2\pi$ to $10000 \cdot 2\pi$.</li> </ul> <p><strong>Why this is clever:</strong></p> <ol> <li><strong>Fixed &amp; Deterministic:</strong> It adds no parameters to the model size.</li> <li><strong>Linear Relationships:</strong> The authors hypothesized that this allows the model to easily attend by relative positions, because for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$ using a rotation matrix. Let $\omega_i$ be the frequency term for dimension $i$,</li> </ol> \[\begin{aligned} \begin{bmatrix} \sin(\omega_i(pos+k)) \\ \cos(\omega_i(pos+k)) \end{bmatrix} = \begin{bmatrix} \cos(\omega_i k) &amp; \sin(\omega_i k) \\ -\sin(\omega_i k) &amp; \cos(\omega_i k) \end{bmatrix} \begin{bmatrix} \sin(\omega_i pos) \\ \cos(\omega_i pos) \end{bmatrix} \end{aligned} \tag{3}\] <ol> <li><strong>Bounded:</strong> The values are always between -1 and 1 (stable training).</li> <li><strong>Unique:</strong> No two positions look the same since its geometric progression makes a large coordinate space. Note that, if linear progression of the wavelength, higher dimension may have the same position embedding as the lower dimension due to sin/cos’s repeatative patterns for long contexts.</li> </ol> <p><strong>NOTE</strong>: Mathematically, the sinusoidal method has no hard limit in sequence length because the equation $PE_{(pos, i)}$ accepts any pos as the input. The code will not crash, and the model will run. However, in practice, if you train a Transformer with sinusoidal embeddings on a context length of 1024, and then test it on length 2048, the performance usually collapses perfectly. <strong>The Unseen Signal Problem</strong>: Even though the sin/cos function stays between -1 and 1, the combination of values across the 512 dimensions creates a specific “fingerprint” for every position.</p> <ul> <li>Training Phase: The model learns weights ($W_Q, W_K, W_V$) based on the “fingerprints” of positions 0 to 1024. It learns how to react when it sees the specific patterns of sine/cosine associated with those numbers.</li> <li>Inference Phase: Suddenly, you feed it position 2000. This position has a “fingerprint” (a specific combination of phases across dimensions) that the model has never seen.</li> <li>Result: The neural network treats this as “out of distribution” noise. It generates garbage query/key projections, the attention mechanism gets confused, and the perplexity explodes.</li> </ul> <h3 id="learned-position-embeddings">Learned Position Embeddings</h3> <p>While the sinusoidal approach is elegant, later models like BERT and the early GPT series took a “brute force” approach that is often easier to implement.</p> <p><strong>How it works</strong></p> <ol> <li>The model initializes a standard matrix of size , where is the maximum context length (e.g., 512 for BERT, 2048 for GPT-3).</li> <li>These vectors are treated as <strong>trainable parameters</strong>.</li> <li>During training, the model learns the “best” vector to represent position 1, position 2, etc., via backpropagation, exactly the same way it learns word meanings.</li> </ol> <p><strong>The Trade-off</strong></p> <ul> <li><strong>Pros:</strong> Conceptually simple and highly effective within the training window.</li> <li><strong>Cons:</strong> It cannot extrapolate. If you train a model with a max length of 512, the model has literally never seen a position embedding for index 513. If you feed it a longer sequence, it crashes or outputs nonsense.</li> </ul> <h3 id="how-it-is-applied">How It Is Applied</h3> <p>Regardless of whether you use the Sinusoidal or Learned method, the application is usually identical. The position vector is <strong>added</strong> (element-wise) to the token embedding before entering the first Transformer layer:</p> <p>This “stamps” the token with information about where it sits in the sentence, allowing the Attention mechanism to differentiate between the first “The” and the second “The” in a sentence.</p> <h2 id="relative-position-embedding">Relative Position Embedding</h2> <p>While Absolute Position Embeddings (both Sinusoidal and Learned) are effective, they share a fundamental flaw: they treat position as a fixed address. But in language, the absolute address doesn’t matter as much as the relative distance. The word “dog” (at index 500) relates to the word “barked” (at index 505) exactly the same way “dog” (at index 5) relates to “barked” (at index 10). The relationship is defined by the distance ($+5$), not the coordinate.</p> <h3 id="the-bridge-relative-position-embeddings-rpe">The Bridge: Relative Position Embeddings (RPE)</h3> <p>To solve this, researchers (Shaw et al., 2018 <d-cite key="shaw2018rpe"></d-cite>; Raffel et al., 2020 <d-cite key="raffel2020t5"></d-cite>) proposed Relative Position Embeddings. Instead of adding a vector to the input tokens, RPE modifies the Attention Mechanism itself. When the model calculates the attention score between query $i$ and key $j$, it explicitly adds a bias term representing the distance $i - j$:</p> \[\text{Attention}(Q, K) = \text{Softmax}\left( \frac{Q K^T + \text{Bias}_{\text{distance}}}{\sqrt{d_k}} \right) V \tag{4}\] <p><strong>The Problem with RPE</strong>: While accurate, standard RPE is computationally expensive. It often requires materializing massive $N \times N$ ($N$ is the sequence length) matrices to store these bias terms, or it complicates the optimized attention kernels (like FlashAttention). We needed a method that had the efficiency of Absolute Embeddings (just modifying the vectors once) but the mathematical properties of Relative Embeddings.</p> <h3 id="rope-rotary-positional-embeddings">RoPE: Rotary Positional Embeddings</h3> <p>Introduced by Su et al. (2021) <d-cite key="su2024roformer"></d-cite>, RoPE is the position embedding method of choice for modern LLMs, including Llama 2, Llama 3, Mistral, and PaLM. The Core Intuition: Rotation, Not Addition. Previous methods (Sinusoidal and Learned) moved the embedding vector by adding a position vector to it:</p> \[\boldsymbol{x}' = \boldsymbol{x} + \boldsymbol{p} \tag{5}\] <p>RoPE takes a different approach. It encodes position by rotating the vector in geometric space.</p> \[\boldsymbol{x}' = \boldsymbol{R}_{pos} \cdot \boldsymbol{x} \tag{6}\] <p>Why rotation? Because in 2D space, if you have a vector at angle $\theta$ and you rotate it by $\phi$, the new angle is simply $\theta + \phi$. Rotation is inherently additive in angles, which preserves relative information perfectly when we take the dot product.</p> <h4 id="the-math-how-rope-works">The Math: How RoPE Works</h4> <p>RoPE treats the embedding vector of size $d$ not as a single chunk, but as $d/2$ pairs of numbers. Each pair is treated as a coordinate $(x, y)$ in a 2D plane. For a token at position $m$, we rotate each pair by an angle $m \cdot \theta_i$, where $\theta_i$ is the frequency for that specific $i^{th}$ dimension. Using complex numbers, this is elegantly simple. For a 2D vector represented as a complex number $q$:\(f(q, m) = q \cdot e^{im\theta}\)In linear algebra terms (real numbers), this is a rotation matrix multiplication. For a feature pair $(q_1, q_2)$ at position $m$:</p> \[\begin{pmatrix} q'_1 \\ q'_2 \end{pmatrix} = \begin{pmatrix} \cos(m\theta) &amp; -\sin(m\theta) \\ \sin(m\theta) &amp; \cos(m\theta) \end{pmatrix} \begin{pmatrix} q_1 \\ q_2 \end{pmatrix} \tag{7}\] <h4 id="the-relative-magic-the-dot-product">The “Relative” Magic (The Dot Product)</h4> <p>The reason RoPE took over the world is what happens when two rotated vectors interact in the Self-Attention layer. Let’s look at the dot product between a Query at position $m$ and a Key at position $n$. $\boldsymbol{q}_m$ is rotated by angle $m\theta$. $\boldsymbol{k}_n$ is rotated by angle $n\theta$. When we take their dot product (which measures similarity):</p> \[\langle \boldsymbol{q}_m, \boldsymbol{k}_n \rangle = \text{Real}( (\boldsymbol{q} e^{im\theta}) \cdot (\boldsymbol{k} e^{in\theta})^* ) \tag{8}\] <p>Using exponent rules ($e^A \cdot e^{-B} = e^{A-B}$), the absolute positions $m$ and $n$ cancel out, leaving only the difference:</p> \[\langle \boldsymbol{q}_m, \boldsymbol{k}_n \rangle = \langle \boldsymbol{q}, \boldsymbol{k} \rangle \cos((m-n)\theta) + \dots \tag{9}\] <p>The attention score depends only on the relative distance $(m-n)$. The model naturally understands “5 steps back” regardless of whether it’s at step 100 or step 1000.</p> <p>In practice, RoPE is applied efficiently:</p> <ul> <li>Do not touch the value vectors ($V$).</li> <li>Split the Query ($Q$) and Key ($K$) vectors into pairs.</li> <li>Rotate each pair by its specific frequency angle $\theta_i$ multiplied by the position index $m$.</li> <li>Feed these rotated $Q$ and $K$ into standard attention.</li> </ul> <p>Note: While RoPE extrapolates better than learned embeddings, extending it to massive lengths still requires tricks like “NTK-Aware Scaling” or “Linear Scaling,” which are simple adjustments to the rotation frequency.</p> <h3 id="why-relative-bias-extrapolates-perfectly-the-bucket-trick">Why Relative Bias Extrapolates “Perfectly” (The Bucket Trick)</h3> <p>You might ask: “If the Relative Bias approach is so inefficient, why did models like T5 use it?”</p> <p>The answer is that it offers a very simple, robust guarantee for extrapolation. It solves the “unknown position” problem by simply refusing to distinguish between long distances.</p> <ol> <li>Translation Invariance (The Foundation)</li> </ol> <p>First, like RoPE, the Relative Bias method relies on the distance $i-j$, not the absolute positions. The model learns a parameter for “distance 5.” It applies that parameter whether the tokens are at indices (10, 15) or indices (1000, 1005). This means it inherently understands that the local structure of language is the same everywhere in the document.</p> <ol> <li>The “Clipping” or “Bucketing” Trick</li> </ol> <p>The real secret to its extrapolation capability is how it handles the “infinite” tail of potential distances. In the original paper (Shaw et al.) and T5, they don’t learn a unique bias for every integer to infinity. Instead, they clip the distance at a certain maximum (let’s say $k=128$).</p> \[\text{used\_distance} = \min(|i - j|, k) \tag{10}\] <p>This acts as a “catch-all” bucket.</p> <ul> <li>Distance 5: Uses the learned bias $b_5$.</li> <li>Distance 50: Uses the learned bias $b_{50}$.</li> <li>Distance 128: Uses the learned bias $b_{128}$.</li> <li>Distance 5,000,000: Also uses the learned bias $b_{128}$.</li> </ul> <ol> <li>Why This Enables Extrapolation</li> </ol> <p>Imagine you train a model on sequences of length 512. The model learns precise relationships for distances 0–128, and a generic relationship for “anything further than 128.</p> <p>“When you deploy this model on a document with 10,000 tokens:</p> <ul> <li>Short Range: The model sees two words 5 steps apart. It uses $b_5$ (which it knows well).</li> <li>Long Range: The model sees two words 5,000 steps apart. It checks its lookup table. It doesn’t look for entry “5,000” (which doesn’t exist). It defaults to the bucket “128+”.</li> <li>Result: It applies the learned bias for “far away.”</li> </ul> <p>The model never encounters an “unknown” state. It simply categorizes all new, ultra-long distances into the “far away” category it already learned during training.</p> <h4 id="the-near-far-analogy">The “Near-Far” Analogy</h4> <p>Think of how you perceive objects:</p> <ul> <li>1 meter away: You see it clearly (Distance 1).</li> <li>10 meters away: You see it somewhat clearly (Distance 10).</li> <li>100 meters away: It’s blurry (Distance 100).</li> <li>1 mile away: It’s a speck.</li> <li>100 miles away: It’s also a speck.</li> </ul> <p>Relative Bias works the same way. Once a word is “far enough” (past the clipping point $k$), the model stops caring about the exact meter-by-meter distance and just treats it as “background context.” This allows it to handle infinite lengths without crashing.</p> <h3 id="scaling-rope-extending-the-context-horizon">Scaling RoPE: Extending the Context Horizon</h3> <p>While <strong>Rotary Positional Embeddings (RoPE)</strong> are naturally more flexible than absolute embeddings, they aren’t magic. If you train a model on <strong>1,024 tokens</strong> and suddenly feed it <strong>2,048</strong>, the attention mechanism will likely collapse. This happens because the model encounters rotation angles it never saw during training.</p> <p>To fix this without a costly full retraining, researchers use two primary scaling “tricks”: <strong>Linear Scaling</strong> and <strong>NTK-Aware Scaling.</strong></p> <h4 id="linear-scaling-position-interpolation">Linear Scaling (Position Interpolation)</h4> <p>Linear scaling is the “rubber band” approach. It stretches the original training range across the new, longer sequence.</p> <ul> <li><strong>The Logic:</strong> If $L$ is the original length and $L’$ is the new length, we define a scale $s=L’/L$. We then divide every position index $n$ by $s$.</li> <li><strong>Example:</strong> To move from 1,024 to 2,048 tokens ($s=2$), token 2000 is treated as token 1000.</li> <li><strong>The Trade-off:</strong> This “squishes” the positions together. By diluting the resolution, the model may lose the ability to distinguish between very close tokens, making its “vision” blurry.</li> </ul> <h4 id="ntk-aware-scaling">NTK-Aware Scaling</h4> <p>NTK-Aware scaling is a more surgical method. Instead of scaling the position index, we scale the <strong>base frequency</strong> ($b$) of the RoPE calculation.</p> <ul> <li><strong>The Math:</strong> We transform the original base (usually 10,000) into a new base $b’$:</li> </ul> \[\begin{aligned} b' = b \cdot s^{\frac{d}{d-2}} \\ \theta_i = b'^{-2i/d} \end{aligned} \tag{11}\] <ul> <li><strong>The Result:</strong> This modification is <strong>non-uniform</strong>. It stretches the long-range (low-frequency) dimensions significantly while leaving the short-range (high-frequency) dimensions almost untouched.</li> </ul> <p><strong>Why High-Frequency Dims Don’t Break</strong></p> <p>A common question arises: <em>If we are at token 2,048, the high-frequency dimensions will produce an absolute angle the model never saw in training. Why doesn’t this cause an error?</em></p> <p>The answer lies in two properties of the attention mechanism:</p> <ul> <li><strong>Periodic Wrapping:</strong> High-frequency dimensions (for small $i$) are like the “second hand” on a watch. The rotation $\theta_i$ is close to 1 ($i=0 \rightarrow \theta_i = 1$). They spin so fast that they complete many full circles within the first 1,024 tokens. E.g., the rotation angle for token at position 1024 is $1024 \cdot \theta_i \approx 1024 \text{ for small } i $. By token 2,048, the hand might have spun 200 times instead of 100, but the <strong>final angle</strong> it lands on is still a value the model has seen thousands of times.</li> <li><strong>Relative Distance Preservation:</strong> Attention scores are calculated based on the <em>difference</em> between angles. At position 2,048, the angular difference between it and position 2,047 is identical to the difference between positions 2 and 1. The model’s “local grammar” stays intact.</li> <li><strong>The Low-Frequency “Hour Hand”:</strong> Low-frequency dimensions are different. They rotate so slowly they might only complete $1/4$ of a circle during training since the rotation $\theta_i$ is very small for larger $i$. If we let them continue to $1/2$ a circle at token 2,048, the model enters “uncharted territory.” <strong>NTK-Aware scaling</strong> effectively slows this hand down by using a larger base $b’$, keeping it within the 1/4-circle range the model understands or has seen during training.</li> </ul> <p><em>Disclosure: This post was drafted with the assistance of an AI language model</em></p>]]></content><author><name>Bo Peng</name></author><category term="position"/><category term="embedding,"/><category term="sinusoidal,"/><category term="rope"/><summary type="html"><![CDATA[Overview of position embedding methods used in LLMs]]></summary></entry><entry><title type="html">Tutorial - A Diffusion Model for Image Restoration</title><link href="https://ibopeng.github.io/blog/2024/irsde/" rel="alternate" type="text/html" title="Tutorial - A Diffusion Model for Image Restoration"/><published>2024-11-11T00:00:00+00:00</published><updated>2024-11-11T00:00:00+00:00</updated><id>https://ibopeng.github.io/blog/2024/irsde</id><content type="html" xml:base="https://ibopeng.github.io/blog/2024/irsde/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In this post, we talk about generative models for image restoration, taking <strong>Image Restoration with Mean-Reverting Stochastic Differential Equations (IR-SDE)</strong> <d-cite key="luo2023image"></d-cite> as an example, a score-based diffusion model for various low-level vision tasks including denoising, super resolution, deraining, etc.</p> <h2 id="background-of-score-based-diffusion-models-through-sdes">Background of Score-based Diffusion Models through SDEs</h2> <p>First, let’s briefly review the basic framework of score-based diffusion models. Let $\mathbf{x}_0 \sim p_0(\mathbf{x})$ denote a sample $\mathbf{x}_0 \in \mathbb{R}^d$ drawn from the initial ground truth data distribution $p_0(\mathbf{x})$. After adding a small amount of noise to $\mathbf{x}_0$, we have a slightly noise-perturbed sample $\mathbf{x}_1 \in \mathbb{R}^d$. By repeating this step for $N$ (e.g., $N$ could be thousands) steps, we obtain a sequence of noisy samples $\{ \mathbf{x}_i \}_{i=0}^N \in \mathbb{R}^d$ indexed by the step variable $i$.</p> <p>Generalizing the discrete step variable $i$ to a continuous time variable $t$, we have a continuous forward diffusion process $\{ \mathbf{x}(t) \}_{t=0}^T \in \mathbb{R}^d$. With sufficiently large $T$, $\mathbf{x}(T)$ is completely noisy without any information about the initial data $\mathbf{x}(0)$.</p> <p><img src="/assets/img/posts/diffusion_forward.png" alt="Forward Diffusion Process" style="width:100%; height:auto; display:block; margin-left:auto; margin-right:auto;"/></p> <p>Note that the initial data distribution $p_0(\mathbf{x})$ is often intractable. However, given a well-defined forward diffusion process, the final noisy sample distribution $\mathbf{x}(T) \sim p_T(\mathbf{x})$ can be in a tractable form (e.g., a standard Gaussian distribution $\mathcal{N}(\mathbf{0}, \mathbf{I})$).</p> <p>To generate samples from such an intractable data distribution $p_0(\mathbf{x})$, a potential way is to reverse the forward diffusion process such that we sample $\mathbf{x}(T)$ from a tractable prior distribution $p_T(\mathbf{x})$, then generate intermediate noisy data $\mathbf{x}(t) \sim p_t(\mathbf{x})$, and finally the clean data $\mathbf{x}(0) \sim p_0(\mathbf{x})$ through a tractable reverse diffusion process running backwards in time, $T \to 0$.</p> <p>In order to achieve this goal, we often start with a tractable <em>prior distribution</em> $p_T(\mathbf{x})$ (e.g., a standard Gaussian distribution $\mathcal{N}(\mathbf{0}, \mathbf{I})$). The forward diffusion process is modeled by a stochastic differential equation (SDE) to perturbe data such that the final output will end up being completely noisy and follow the prior distribution $p_T(\mathbf{x})$.</p> <p>Formally, the SDE for the forward diffusion process can be constructed in the form of:</p> <p>\begin{equation} \label{eq:forward} \mathrm{d} \mathbf{x} = \mathbf{f}(\mathbf{x}, t) \mathrm{d} t + g(t)\mathrm{d} \mathbf{w} \end{equation}</p> <p>where $\mathbf{w}$ is the standard Wiener process (a.k.a., Brownian motion) (time flow: $0 \to T$), $\mathrm{d}t$ is an infinitesimal positive time interval, and $f(\mathbf{x}, t)$ and $g(t)$ are called the <em>drift</em> and <em>diffusion</em> coefficients of $\mathbf{x}$, respectively. Solving the above forward SDE, the intermediate noise-perturbed sample $\mathbf{x}(t)$ can be obtained given the initial data sample $\mathbf{x}(0)$.</p> <p>Note that, the actual SDE can be in different forms such that the prior distribution $p_T(\mathbf{x})$ will have different forms, rather than just limited to standard Gaussiance distribution $\mathcal{N}(\mathbf{0}, \mathbf{I})$. However, we do not have to solve the forward SDE as our goal is to reverse the forward diffusion process or the SDE for generating data $\mathbf{x}(0)$ starting from the noise/prior $\mathbf{x}(T) \sim p_T(\mathbf{x})$:</p> <p><img src="/assets/img/posts/diffusion_backward.png" alt="Backward Diffusion Process" style="width:100%; height:auto; display:block; margin-left:auto; margin-right:auto;"/></p> <p>An important property of the diffusion process modeled by the above SDE (Eq. \eqref{eq:forward}): <em>its reverse process running backwards in time is also a diffusion process modeled by a reverse-time SDE corresponding to the forward SDE</em>.</p> <p>\begin{equation} \label{eq:reverse} \mathrm{d}\mathbf{x} = [ \mathbf{f}(\mathbf{x}, t) - g(t)^2 \underbrace{ \nabla_{\mathbf{x}}\log{p_t(\mathbf{x})} }_{\text{score: } s_{\theta}(\mathbf{x}, t)} ] \mathrm{d}t + g(t)\mathrm{d}\mathbf{\bar{w}} \end{equation}</p> <p>where $\mathbf{\bar{w}}$ is a standard reverse-time Wiener process (time flow: $T \to 0$), $\mathrm{d}t$ is an infinitesimal negative time interval, and $s_{\theta}(\mathbf{x}, t)=\nabla_{\mathbf{x}}\log{p_t(\mathbf{x})}$ is called the score of the marginal distribution $p_t(\mathbf{x})$ for $\mathbf{x}(t)$ at time $t$. Note that the score function $s_{\theta}(\mathbf{x}, t)$ is often intractable and needs to be approximated by a neural network learned from data.</p> <h2 id="image-restoration-with-mean-reverting-sde">Image Restoration with Mean-reverting SDE</h2> <p>In diffusion models, given a sample from the prior distribution, $\mathbf{x}(T) \sim p_T(\mathbf{x})$, we can generate data from the data distribution, $\mathbf{x}(0) \sim p_0(\mathbf{x})$, through the reverse-time SDE. In the problem of image restoration, given the low-quality (LQ) data (e.g., low-resolution images), the goal is to generate high-quality (HQ) data (e.g., high-resolution images). Corresponding to the framework of the SDE-based diffusion processes, we have:</p> <ul> <li>LQ: <em>known</em> prior distribution, $\mathbf{x}(T) \sim p_T(\mathbf{x})$,</li> <li>HQ: <em>unknown</em> data distribution, $\mathbf{x}(0) \sim p_0(\mathbf{x})$</li> </ul> <h3 id="mean-reverting-sde">Mean-reverting SDE</h3> <p>To adapt to the SDE-based diffusion model framework, we need to design a SDE that perturbes the HQ sample $\mathbf{x}(0)$ such that the final noisy counterpart $\mathbf{x}(T)$ follow the same distribution of LQ data.</p> <p>A special form of the forward SDE <d-cite key="luo2023image"></d-cite> was hand designed to satisfy this prerequisite. Let</p> \[\begin{equation} \begin{aligned} \mathbf{f}(\mathbf{x}, t) &amp;= \theta_t(\boldsymbol{\mu} - \mathbf{x}) \\ g(t) &amp;= \sigma_t \end{aligned} \end{equation}\] <p>We have a special SDE:</p> \[\begin{equation} \label{eq:irsde} \mathrm{d}\mathbf{x} = \theta_t(\boldsymbol{\mu} - \mathbf{x}) \mathrm{d}t + \sigma_t \mathrm{d} \mathbf{w} \end{equation}\] <p>where \(\mu\) is the LQ sample, $\theta_t$ and $\sigma_t$ are time-dependent hyperparameters. To sovle the SDE (Eq. \eqref{eq:irsde}), let’s first define a function</p> \[\begin{equation} \phi(\mathbf{x}, t) = \mathbf{x} \mathrm{e}^{\bar{\theta}_t} \end{equation}\] <p>where $\mathbf{x}(t) \sim p_t(\mathbf{x})$ is the data sample at time $t$ and $\bar{\theta}_t=\int_0^t \theta_z \mathrm{d}z$. We can get the differential equation of $\phi(\mathbf{x}, t)$:</p> \[\begin{equation} \mathrm{d} \phi(\mathbf{x}, t) = \mathbf{x} \mathrm{e}^{\bar{\theta}_t} \frac{\partial \bar{\theta}_t}{\partial t} \mathrm{d}t + \mathrm{e}^{\bar{\theta}_t} \mathrm{d} \mathbf{x} \end{equation}\] <p>Note that $\bar{\theta}_t=\int_0^t \theta_z \mathrm{d}z$, $\frac{\partial \bar{\theta}_t}{\partial t} = \theta_t$. Also substituting $\mathrm{d} \mathbf{x}$ with Eq. \eqref{eq:irsde}, we have</p> \[\begin{equation} \begin{aligned} \mathrm{d} \phi(\mathbf{x}, t) &amp;= \mathbf{x} \mathrm{e}^{\bar{\theta}_t} \frac{\partial \bar{\theta}_t}{\partial t} \mathrm{d}t + \mathrm{e}^{\bar{\theta}_t} \mathrm{d} \mathbf{x} \\ &amp;= \mathbf{x} \mathrm{e}^{\bar{\theta}_t} \theta_t \mathrm{d}t + \mathrm{e}^{\bar{\theta}_t} [ \theta_t(\boldsymbol{\mu} - \mathbf{x}) \mathrm{d}t + \sigma_t \mathrm{d} \mathbf{w}_t] \\ &amp;= [\cancel{\mathbf{x} \theta_t \mathrm{e}^{\bar{\theta}_t}} + \boldsymbol{\mu} \theta_t \mathrm{e}^{\bar{\theta}_t} - \cancel{\mathbf{x} \theta_t \mathrm{e}^{\bar{\theta}_t}}] \mathrm{d}t + \sigma_t \mathrm{e}^{\bar{\theta}_t} \mathrm{d} \mathbf{w}_t \\ &amp;= \boldsymbol{\mu} \theta_t \mathrm{e}^{\bar{\theta}_t} \mathrm{d}t + \sigma_t \mathrm{e}^{\bar{\theta}_t} \mathrm{d} \mathbf{w}_t \end{aligned} \end{equation}\] <p>where we change $\mathbf{w}$ to $\mathbf{w}_t$ to explicitly express its change over time $t$. As such, we have the relationship between $\phi(\mathbf{x}, t)$ and $\phi(\mathbf{x}, s)$ at any two different times $s &lt; t$:</p> \[\begin{equation} \begin{aligned} \phi(\mathbf{x}, t) - \phi(\mathbf{x}, s) &amp;= \int_s^t \mathrm{d} \phi(\mathbf{x}, z) \\ &amp;= \int_s^t (\boldsymbol{\mu} \theta_z \mathrm{e}^{\bar{\theta}_z} \mathrm{d}z + \sigma_z \mathrm{e}^{\bar{\theta}_z} \mathrm{d} \mathbf{w}_z) \\ &amp;= \int_s^t \boldsymbol{\mu} \theta_z \mathrm{e}^{\bar{\theta}_z} \mathrm{d}z + \int_s^t \sigma_z \mathrm{e}^{\bar{\theta}_z} \mathrm{d} \mathbf{w}_z \\ \end{aligned} \end{equation}\] <p>Substitute $\phi(\mathbf{x}, t)$ with $\mathbf{x} \mathrm{e}^{\bar{\theta}_t}$ and divide both sides of the above equation by $\mathrm{e}^{\bar{\theta}_t}$, we have</p> \[\begin{equation} \label{eq:xts} \begin{aligned} \mathbf{x}(t) \mathrm{e}^{\bar{\theta}_t} - \mathbf{x}(s) \mathrm{e}^{\bar{\theta}_s} &amp;= \int_s^t \boldsymbol{\mu} \theta_z \mathrm{e}^{\bar{\theta}_z} \mathrm{d}z + \int_s^t \sigma_z \mathrm{e}^{\bar{\theta}_z} \mathrm{d} \mathbf{w}_z \\ \Longrightarrow \mathbf{x}(t) - \mathbf{x}(s) \mathrm{e}^{\bar{\theta}_s - \bar{\theta}_t} &amp;= \int_s^t \boldsymbol{\mu} \theta_z \mathrm{e}^{\bar{\theta}_z - \bar{\theta}_t} \mathrm{d}z + \int_s^t \sigma_z \mathrm{e}^{\bar{\theta}_z - \bar{\theta}_t} \mathrm{d} \mathbf{w}_z \end{aligned} \end{equation}\] <p>Recall that $\bar{\theta}_t=\int_0^t \theta_z \mathrm{d}z$. Given $0 &lt; s &lt; t$, we have</p> \[\begin{equation} \begin{aligned} \mathrm{e}^{\bar{\theta}_s - \bar{\theta}_t} &amp;= \mathrm{e}^{\int_0^s \theta_z \mathrm{d}z - \int_0^t \theta_z \mathrm{d}z} \\ &amp;= \mathrm{e}^{-\int_s^t \theta_z \mathrm{d}z}\\ &amp; = \mathrm{e}^{-\bar{\theta}_{s:t}} \end{aligned} \end{equation}\] <p>Rewriting the above Eq. \eqref{eq:xts}, we obtain</p> \[\begin{equation} \begin{aligned} \mathbf{x}(t) - \mathbf{x}(s) \mathrm{e}^{-\bar{\theta}_{s:t}} &amp;= \int_s^t \boldsymbol{\mu} \theta_z \mathrm{e}^{-\bar{\theta}_{z:t}} \mathrm{d}z + \int_s^t \sigma_z \mathrm{e}^{-\bar{\theta}_{z:t}} \mathrm{d} \mathbf{w}_z \end{aligned} \end{equation}\] <p>Note that $\mathrm{d}(-\bar{\theta}_{z:t}) = \mathrm{d} (-\int_z^t \theta_z \mathrm{d}z) = \theta_z$</p> \[\begin{equation} \int_s^t \boldsymbol{\mu} \theta_z \mathrm{e}^{-\bar{\theta}_{z:t}} \mathrm{d}z = \boldsymbol{\mu} \mathrm{e}^{-\bar{\theta}_{z:t}} \bigg|_s^t = \boldsymbol{\mu} (\mathrm{e}^{-\bar{\theta}_{t:t}} - \mathrm{e}^{-\bar{\theta}_{s:t}}) = \boldsymbol{\mu} (1-\mathrm{e}^{-\bar{\theta}_{s:t}}) \end{equation}\] \[\begin{equation} \label{eq:solution_irsde} \begin{aligned} \mathbf{x}(t) &amp;= \mathbf{x}(s) \mathrm{e}^{-\bar{\theta}_{s:t}} + \boldsymbol{\mu} (1-\mathrm{e}^{-\bar{\theta}_{s:t}}) + \int_s^t \sigma_z \mathrm{e}^{-\bar{\theta}_{z:t}} \mathrm{d} \mathbf{w}_z \\ \mathbf{x}(t) &amp;= \boldsymbol{\mu} + (\mathbf{x}(s) - \boldsymbol{\mu}) \mathrm{e}^{-\bar{\theta}_{s:t}} + \int_s^t \sigma_z \mathrm{e}^{-\bar{\theta}_{z:t}} \mathrm{d} \mathbf{w}_z \end{aligned} \end{equation}\] <p>which is the solution to the SDE in Eq. \eqref{eq:irsde}. For any starting sample $\mathbf{x}_s$ at time $s$, we can generate the noise-perturbed sample $\mathbf{x}_t$ at time $t$. Looking into the expectation of the Eq. \eqref{eq:solution_irsde}, we have</p> \[\begin{equation} \begin{aligned} \mathbb{E}[\mathbf{x}(t)] &amp;= \mathbb{E}\left[\boldsymbol{\mu} + (\mathbf{x}(s) - \boldsymbol{\mu}) \mathrm{e}^{-\bar{\theta}_{s:t}} + \int_s^t \sigma_z \mathrm{e}^{-\bar{\theta}_{z:t}} \mathrm{d} \mathbf{w}_z \right] \\ &amp;= \mathbb{E}\left[\boldsymbol{\mu} + (\mathbf{x}(s) - \boldsymbol{\mu}) \mathrm{e}^{-\bar{\theta}_{s:t}} \right] + \mathbb{E}\left[\int_s^t \sigma_z \mathrm{e}^{-\bar{\theta}_{z:t}} \mathrm{d} \mathbf{w}_z \right] \\ \end{aligned} \end{equation}\] <p>Since the first part $\mu + (\mathbf{x}(s) - \mu) \mathrm{e}^{-\bar{\theta}_{s:t}}$ is deterministic, its expectation is itself</p> \[\begin{equation} \mathbb{E}\left[\boldsymbol{\mu} + (\mathbf{x}(s) - \boldsymbol{\mu}) \mathrm{e}^{-\bar{\theta}_{s:t}} \right] = \boldsymbol{\mu} + (\mathbf{x}(s) - \boldsymbol{\mu}) \mathrm{e}^{-\bar{\theta}_{s:t}} \end{equation}\] <p>Regarding the second part $\mathbb{E}\left[\int_s^t \sigma_z \mathrm{e}^{-\bar{\theta}_{z:t}} \mathrm{d} \mathbf{w}_z \right]$, let’s first review some Itô basics.</p> <hr/> <p><strong>Theorem</strong>. Let $\mathbf{w}_t$ be a Wiener process, and $\Delta(t)$ be a nonrandom function of time. Suppose that a stochastic process $\mathbf{h}(t)$ satisfies</p> \[\begin{equation} \mathrm{d} \mathbf{h} = \Delta(z)\mathrm{d}\mathbf{w}_z \\ \mathbf{h}(t) = \int \Delta(z) \mathrm{d}\mathbf{w}_z \end{equation}\] <p>where $\mathbf{h}(0)=0$. Then, for each $t&gt;=0$, the random variable $\mathbf{h}(t)$ is normally distributed.</p> <hr/> <p>Let $\Delta(z) = \sigma_z \mathrm{e}^{-\bar{\theta}_{z:t}}$ which is nonrandom, $\mathbf{h}(t) = \int_s^t \Delta(z) \mathrm{d} \mathbf{w}_z$, we have</p> \[\begin{equation} \mathbb{E}\left[\int_s^t \sigma_z \mathrm{e}^{-\bar{\theta}_{z:t}} \mathrm{d} \mathbf{w}_z \right] = \mathbb{E}\left[\int_s^t \Delta(z) \mathrm{d} \mathbf{w}_z \right] = \mathbb{E}[\mathbf{h}(t)] = \mathbf{0} \end{equation}\] <p>We can further derive the covariance of $I(t)$.</p> \[\begin{equation} \begin{aligned} \mathrm{Cov}\left(\mathbf{h}(t)\right) &amp;= \mathbb{E} \left[ \left(\mathbf{h}(t)-\mathbb{E}[\mathbf{h}(t)] \right) \left(\mathbf{h}(t)-\mathbb{E}[\mathbf{h}(t)] \right)^\top \right] \\ &amp;= \mathbb{E} \left[ \mathbf{h}(t) \mathbf{h}(t)^\top \right]\\ &amp;= \mathbb{E}\left[ \left(\int_s^t \Delta(z) \mathrm{d} \mathbf{w}_z\right) \left(\int_s^t \Delta(z) \mathrm{d} \mathbf{w}_z\right)^\top \right]\\ \end{aligned} \end{equation}\] <p>Since $\mathbf{h}(t) = \int_s^t \Delta(z) \, \mathrm{d} \mathbf{w}_z$, we can apply the Itô isometry to compute the covariance matrix. For any two components $\mathbf{h}^{(i)}(t)$ and $\mathbf{h}^{(j)}(t)$ of $\mathbf{h}(t)$, we have:</p> \[\begin{equation} \mathbb{E}\left[ \mathbf{h}^{(i)}(t) \mathbf{h}^{(j)}(t) \right] = \mathbb{E}\left[ \int_s^t \Delta(z) \, \mathrm{d} \mathbf{w}_z^{(i)} \int_s^t \Delta(z) \, \mathrm{d} \mathbf{w}_z^{(j)} \right]. \end{equation}\] <p>Since the Wiener processes $\mathbf{w}_z^{(i)}$ and $\mathbf{w}_z^{(j)}$ are independent for $i \neq j$, the cross terms $\mathbb{E}\left[ \mathbf{h}^{(i)}(t) \mathbf{h}^{(j)}(t) \right] = 0$ when $i \neq j$. For the diagonal terms where $i = j$, the Itô isometry gives:</p> \[\begin{equation} \mathbb{E}\left[ \left( \mathbf{h}^{(i)}(t) \right)^2 \right] = \mathbb{E}\left[ \left( \int_s^t \Delta(z) \, \mathrm{d} \mathbf{w}_z^{(i)} \right)^2 \right] = \int_s^t \Delta(z)^2 \, \mathrm{d}z. \end{equation}\] <p>Thus, the covariance matrix $\text{Cov}(\mathbf{h}(t))$ is a diagonal matrix, where each diagonal entry is $\int_s^t \Delta(z)^2 \, \mathrm{d}z$.</p> \[\begin{equation} \begin{aligned} \mathrm{Cov}\left(\mathbf{h}(t)\right) &amp;= \mathbb{E}\left[\left(\int_s^t \Delta(z)^2 \mathrm{d}z\right)\right] \cdot \mathbf{I} \quad (\mathbf{I} \text{ is identity matrix})\\ &amp;= \mathbb{E}\left[\left(\int_s^t \left(\sigma_z \mathrm{e}^{-\bar{\theta}_{z:t}}\right)^2 \mathrm{d}z\right)\right] \cdot \mathbf{I}\\ &amp;= \mathbb{E}\left[\left(\int_s^t \sigma_z^2 \mathrm{e}^{-2\bar{\theta}_{z:t}} \mathrm{d}z\right)\right] \cdot \mathbf{I}\\ &amp;= \mathbb{E}\left[ \frac{\sigma_z^2}{2\theta_z} \mathrm{e}^{-2\bar{\theta}_{z:t}} \bigg|_s^t \right] \cdot \mathbf{I}\\ &amp;= \mathbb{E}\left[ \frac{\sigma_t^2}{2\theta_t} \mathrm{e}^{-2\bar{\theta}_{t:t}} - \frac{\sigma_s^2}{2\theta_s} \mathrm{e}^{-2\bar{\theta}_{s:t}} \right] \cdot \mathbf{I}\\ &amp;= \mathbb{E}\left[ \frac{\sigma_t^2}{2\theta_t} \mathrm{e}^0 - \frac{\sigma_s^2}{2\theta_s} \mathrm{e}^{-2\bar{\theta}_{s:t}} \right] \cdot \mathbf{I}\\ &amp;= \mathbb{E}\left[ \lambda^2 - \lambda^2 \mathrm{e}^{-2\bar{\theta}_{s:t}} \right] \cdot \mathbf{I} \quad (\sigma_t^2 / \theta_t = 2\lambda^2, 0 &lt; t &lt; T) \\ &amp;= \lambda^2 \left( 1- \mathrm{e}^{-2\bar{\theta}_{s:t}} \right) \cdot \mathbf{I} \end{aligned} \end{equation}\] <p>Therefore, we have</p> \[\begin{equation} \mathbf{h}(t) \sim \mathcal{N} \left( \mathbf{0}, \lambda^2 \left( 1- \mathrm{e}^{-2\bar{\theta}_{s:t}} \right) \cdot \mathbf{I} \right) \end{equation}\] <p>Note that $\mathbf{x}(t) \sim p_t(\mathbf{x})$ also follows Gaussian distribution:</p> \[\begin{equation} \begin{aligned} \mathbf{x}(t) &amp;= \boldsymbol{\mu} + \mathrm{e}^{-\bar{\theta}_{s:t}} \cdot \left[ \mathbf{x}(s) - \boldsymbol{\mu} \right] + \int_s^t \sigma_z \mathrm{e}^{-\bar{\theta}_{z:t}} \mathrm{d} \mathbf{w}_z \\ &amp;= \boldsymbol{\mu} + \mathrm{e}^{-\bar{\theta}_{s:t}} \cdot \left[\mathbf{x}(s) - \boldsymbol{\mu} \right] + \mathbf{h}(t) \\ \end{aligned} \end{equation}\] \[\begin{equation} \label{eq:mu_std} \begin{aligned} p(\mathbf{x}(t) | \mathbf{x}(s)) &amp;= \mathcal{N} \left( \mathbf{x}(t) | \boldsymbol{\mu}_{s:t}(\mathbf{x}(s)), \mathbf{\Sigma}_{s:t} \right) \\ \boldsymbol{\mu}_{s:t}(\mathbf{x}(s)) &amp;= \boldsymbol{\mu} + \mathrm{e}^{-\bar{\theta}_{s:t}} \cdot \left[ \mathbf{x}(s) - \boldsymbol{\mu} \right] \\ \mathbf{\Sigma}_{s:t} &amp;= \text{Cov}(\mathbf{h}(t)) = \lambda^2 \left( 1- \mathrm{e}^{-2\bar{\theta}_{s:t}} \right)\cdot \mathbf{I} \end{aligned} \end{equation}\] <p>We observe that, with $t \to \infty$, the mean of $\mathbf{x}_t$, $\mu_{s:t}(\mathbf{x}_s)$ converges to the LQ sample $\mu$ and the covariance $\Sigma_{s:t}$ converges to $\lambda^2$, which is called the <em>stationary variance</em> in the IR-SDE paper <d-cite key="luo2023image"></d-cite>. Therefore, if starting with the initial HQ sample $\mathbf{x}_s = \mathbf{x}_0$, the hand-designed SDE in Eq. \eqref{eq:irsde} established a framework of an SDE-based diffusion model for image restoration, diffusing $\mathbf{x}_0$ to its noisy counterpart, the LQ sample $\mathbf{x}_T$.</p> <h3 id="reversing-the-sde">Reversing the SDE</h3> <p>By Eq. \eqref{eq:reverse}, the reverse-time SDE corresponding to the SDE (Eq. \eqref{eq:irsde}) is:</p> \[\begin{equation} \label{eq:reverse_irsde_custom} \begin{aligned} \mathrm{d}\mathbf{x} &amp;= [\mathbf{f}(\mathbf{x}, t) - g(t)^2 \nabla_{\mathbf{x}}\log{p_t(\mathbf{x})}]\mathrm{d}t + g(t)\mathrm{d}\mathbf{\bar{w}} \\ &amp;= \left[ \theta_t (\boldsymbol{\mu} - \mathbf{x}) - \sigma_t^2 \nabla_{\mathbf{x}} \log p_t(\mathbf{x}) \right] \mathrm{d}t + \sigma_t \mathrm{d} \bar{\mathbf{w}} \end{aligned} \end{equation}\] <p>With this reverse-time SDE flowing backwards in time $T \to 0$, we can starting from the LQ sample $\mathbf{x}(T) \sim p_T(\mathbf{x})$ and generate the HQ sample $\mathbf{x}(0) \sim p_0(\mathbf{x})$. However, we still have an unknown part, the score $\nabla_{\mathbf{x}} \log p_t(\mathbf{x})$, in the reverse-time SDE.</p> <p>As the score function $\nabla_{\mathbf{x}} \log p_t(\mathbf{x})$ is intractable for real-world data, it is often approximated by a neural network trained with ground-truth HQ-LQ sample pairs, $\left(\mathbf{x}(0), \mathbf{x}(T)\right)$. To train a neural network mode $s_{\theta}(\mathbf{x}(t), t)$ given time $t$ and the intermediate noisy sample $\mathbf{x}(t)$, we first compute the conditional ground-truth score $\nabla_{\mathbf{x}} \log p_t(\mathbf{x} | \mathbf{x}(0))$ given the HQ sample $\mathbf{x}(0)$. Note that</p> \[\begin{equation} \label{eq:prob_s_t} p(\mathbf{x}(t) | \mathbf{x}(s)) = \mathcal{N} \left( \mathbf{x}(t) | \boldsymbol{\mu}_{s:t}(\mathbf{x}(s)), \mathbf{\Sigma}_{s:t} \right) \end{equation}\] <p>we have</p> \[\begin{equation} p_t(\mathbf{x} | \mathbf{x}(0)) = \mathcal{N} \left( \mathbf{x}(t) | \boldsymbol{\mu}_{0:t}(\mathbf{x}(0)), \mathbf{\Sigma}_{0:t} \right) \end{equation}\] <p>To simplify notation, we replace all subscripts $\{ * \}_{0:t}$ with $\{ * \}_{t}$. Thus, we have</p> \[\begin{equation} \begin{aligned} p_t(\mathbf{x} | \mathbf{x}(0)) &amp;= \mathcal{N} \left( \mathbf{x}(t) | \boldsymbol{\mu}_{t}, \mathbf{\Sigma}_{t} \right) \\ &amp;= \frac{1}{(2 \pi)^{d/2} |\mathbf{\Sigma}_{t}|^{1/2}} \exp\left( -\frac{1}{2} \left(\mathbf{x}(t) - \boldsymbol{\mu}_{t}\right)^\top \mathbf{\Sigma}_{t}^{-1} \left(\mathbf{x}(t) - \boldsymbol{\mu}_{t}\right) \right) \end{aligned} \end{equation}\] <p>Hence, the score has the following closed-form solution:</p> \[\begin{equation} \nabla_{\mathbf{x}} \log p_t(\mathbf{x} | \mathbf{x}(0)) = -\mathbf{\Sigma}_{t}^{-1} \left( \mathbf{x}(t) - \boldsymbol{\mu}_{t} \right) \end{equation}\] <h3 id="training-objectives">Training Objectives</h3> <h4 id="noise-prediction">Noise Prediction</h4> <p>With ground-truth clean sample $\mathbf{x}(0)$ in training, we first perturbe $\mathbf{x}(0)$ through the diffusion process via the forward SDE and obtain the noise-perturbed sample $\mathbf{x}(t)$.</p> <p>Next, we compute the mean $\mu_{t}$ and covariance $\Sigma_{t}$ by Eq. \eqref{eq:mu_std} such that we can obtain the ground-truth score $\nabla_{\mathbf{x}}\log{p_t(\mathbf{x}|\mathbf{x}(0))}$ for training the score network $\mathbf{s}_{\theta}(\mathbf{x}(t),t)$.</p> <p>During inference, the reverse-time SDE starts from the final time $T$ such that the score network approximates the score at time $T$ given the most noisy input LQ sample, $\mathbf{x}(T)$. Ideally, after iteratively running backwards in time, $T \to 0$, the reverse process will generate the clean HQ sample, $\mathbf{x}(0)$, via the reverse-time SDE (Eq. \eqref{eq:reverse_irsde_custom}). However, the actual performance might not be as expected, as detailed in many studies.</p> <p>Considering the conditional distribution, $p_t(\mathbf{x} | \mathbf{x}(0)) \sim \mathcal{N}(\mu_t, \Sigma_t)$, we have $\mathbf{x}(t) = \mu_{t} + |\Sigma_t|^{1/2} \epsilon_t$, where $\epsilon_t \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$. The score is reparameterized as</p> \[\begin{equation} \nabla_{\mathbf{x}} \log p_t(\mathbf{x} | \mathbf{x}(0)) = -\mathbf{\Sigma}_{t}^{-1/2} \boldsymbol{\epsilon}_t \end{equation}\] <p>Therefore, we can, instead, train a noise network $\tilde{\epsilon}_{\phi}(\mathbf{x}(t), \mu, t)$ which takes the intermediate noisy sample $\mathbf{x}(t)$, the LQ sample or condition $\mu$, and the time $t$ as input and outputs the noise to approximate the ground-truth noise $\epsilon_t$.</p> <p>The noise network can be trained following a similar framework proposed in the work Denoising Diffusion Probabilistic Models (DDPM) <d-cite key="ho2020denoising"></d-cite> by minimizing the following loss (after discretization from continuous time variable $t \in [0, T]$ to discretized step variable $\{ i\}_{i=0}^N$):</p> \[\begin{equation} L_{\gamma}(\phi) = \sum_{i=1}^{N}\gamma_i \mathbb{E} \left[ \Vert \tilde{\boldsymbol{\epsilon}}_{\phi}(\mathbf{x}_i, \boldsymbol{\mu}, i) - \boldsymbol{\epsilon}_i \Vert \right] \end{equation}\] <p>where, for the $i^{\text{th}}$ step in the diffusion process, $\gamma_i$ denotes the weight, $\mathbf{x}_i$ represents the intermediate noisy sample, and ${\epsilon}_i$ is the ground-truth randomly sampled Gaussian noise.</p> <p>After the noise network $\tilde{\epsilon}_{\phi}(\mathbf{x}_i, {\mu}, i)$ is trained, we can start from the LQ sample $\mathbf{x}_N$ and iteratively solve the reverse-time SDE with numerical approaches (e.g., Euler-Maruyama or stochastic Runge-Kutta methods) for generating the HQ sample $\mathbf{x}_0$. More details can be found in the study, Score-based Diffusion Models through SDE <d-cite key="song2020score"></d-cite>.</p> <h4 id="maximum-likelihood">Maximum Likelihood</h4> <p>Unfortunately, the above objective $L_{\gamma}(\phi)$ did not produce good HQ samples in experiments while there is not good reason in the original paper explaining this issue. Instead, a new maximum likelihood objective was proposed for training the noise network.</p> <p>In this part, let’s the diffusion process at discrete times $\{i\}_{i=0}^N$ and the Markov property. The joint probability of $(\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_N )$ conditioned on $\mathbf{x}_0$ is</p> \[\begin{equation} p(\mathbf{x}_{1:N}|\mathbf{x}_0) = p(\mathbf{x}_N|\mathbf{x}_0) \prod_{i=2}^N p(\mathbf{x}_{i-1}|\mathbf{x}_{i}, \mathbf{x}_0) \end{equation}\] <p>where we consider the intermediate noisy sample $\mathbf{x}_{i-1}$ is conditioned on both the initial HQ sample $\mathbf{x}_0$ and the next noise-perturbed sample $\mathbf{x}_{i}$.</p> <p>Using Bayes’ rule, the reverse transition, $p(\mathbf{x}_{i-1} | \mathbf{x}_i, \mathbf{x}_0)$ can be expressed as</p> \[\begin{equation} \begin{aligned} p(\mathbf{x}_{i-1}|\mathbf{x}_{i}, \mathbf{x}_0) &amp;= \frac{p(\mathbf{x}_{i}|\mathbf{x}_{i-1}, \mathbf{x}_0) p(\mathbf{x}_{i-1}|\mathbf{x}_0)}{p(\mathbf{x}_i|\mathbf{x}_0)} \\ &amp;= \frac{p(\mathbf{x}_{i}|\mathbf{x}_{i-1}) p(\mathbf{x}_{i-1}|\mathbf{x}_0)}{p(\mathbf{x}_i|\mathbf{x}_0)} \end{aligned} \end{equation}\] <p>where each of the above probabilities are Gaussian (Eq. \eqref{eq:prob_s_t}). The goal is to find the optimal $\mathbf{x}_{i-1}^*$ to maximize the likelihood $p(\mathbf{x}_{i-1} | \mathbf{x}_i, \mathbf{x}_0)$ or minimize the negative log likelihood (NLL) $- \log p(\mathbf{x}_{i-1} | \mathbf{x}_i, \mathbf{x}_0)$.</p> \[\begin{equation} \begin{aligned} \mathbf{x}_{i-1}^* &amp;= \arg \max_{\mathbf{x}_{i-1}} \left[ p(\mathbf{x}_{i-1}|\mathbf{x}_{i}, \mathbf{x}_0)\right] \\ &amp;= \arg \max_{\mathbf{x}_{i-1}} \left[ p(\mathbf{x}_{i}|\mathbf{x}_{i-1}) p(\mathbf{x}_{i-1}|\mathbf{x}_0) \right] \\ &amp;= \arg \min_{\mathbf{x}_{i-1}} \left[ -\log p(\mathbf{x}_{i}|\mathbf{x}_{i-1}) - \log p(\mathbf{x}_{i-1}|\mathbf{x}_0) \right] \\ \end{aligned} \end{equation}\] <p>The discretized version of the transition kernel, $p(\mathbf{x}_t | \mathbf{x}_s)$, between times $s$ and $t$, becomes:</p> \[\begin{equation} \label{eq:prob_s_t_discrete} \begin{aligned} p(\mathbf{x}_t | \mathbf{x}_s) &amp;= \mathcal{N} \left( \mathbf{x}_t | \boldsymbol{\mu}_{s:t}(\mathbf{x}_s), \mathbf{\Sigma}_{s:t} \right) \\ \boldsymbol{\mu}_{s:t}(\mathbf{x}_s) &amp;= \boldsymbol{\mu} + \mathrm{e}^{-\bar{\theta}_{s:t}} \cdot \left[ \mathbf{x}_s - \boldsymbol{\mu} \right] \\ \mathbf{\Sigma}_{s:t} &amp;= \text{Cov}(\mathbf{h}_t) = \lambda^2 \left( 1- \mathrm{e}^{-2\bar{\theta}_{s:t}} \right)\cdot \mathbf{I} \end{aligned} \end{equation}\] <p>As the transitions $p(\mathbf{x}_{i} | \mathbf{x}_{i-1})$ and $p(\mathbf{x}_{i-1} | \mathbf{x}_0)$ have been derived in Eq. \eqref{eq:prob_s_t_discrete}, we can solve the above optimization problem by taking the derivative.</p> \[\begin{equation} \begin{aligned} &amp; - \nabla_{\mathbf{x}_{i-1}} \log p(\mathbf{x}_{i}|\mathbf{x}_{i-1}) - \nabla_{\mathbf{x}_{i-1}} \log p(\mathbf{x}_{i-1}|\mathbf{x}_0) \\ = &amp; - \mathbf{\Sigma}_{(i-1):i}^{-1} (\mathbf{x}_{i} - \boldsymbol{\mu}_{(i-1):i})\mathrm{e}^{-\bar{\theta}_{(i-1):i}} - \mathbf{\Sigma}_{0:(i-1)}^{-1} (\mathbf{x}_{i-1} - \boldsymbol{\mu}_{0:(i-1)}) \\ = &amp; -\frac{\mathrm{e}^{-\bar{\theta}_{(i-1):i}}}{\lambda^2 \left( 1- \mathrm{e}^{-2\bar{\theta}_{(i-1):i}} \right)} (\mathbf{x}_{i} - \boldsymbol{\mu} - \mathrm{e}^{-\bar{\theta}_{(i-1):i}} \cdot ( \mathbf{x}_{i-1} - \boldsymbol{\mu} )) \\ &amp; +\frac{1}{\lambda^2 \left( 1- \mathrm{e}^{-2\bar{\theta}_{0:(i-1)}} \right)} (\mathbf{x}_{i-1} - \boldsymbol{\mu} - \mathrm{e}^{-\bar{\theta}_{0:(i-1)}} \cdot \left( \mathbf{x}_0 - \boldsymbol{\mu} \right)) \\ = &amp; 0 \end{aligned} \end{equation}\] <p>Setting the above derivative to 0 and solving the equation, we have:</p> \[\begin{equation} \mathbf{x}_{i-1}^* = \frac{1- \mathrm{e}^{-2\bar{\theta}_{0:(i-1)}}}{1- \mathrm{e}^{-2\bar{\theta}_{0:i}}} \mathrm{e}^{-\bar{\theta}_{(i-1):i}} (\mathbf{x}_{i}-\boldsymbol{\mu}) + \frac{1- \mathrm{e}^{-2\bar{\theta}_{(i-1):i}}}{1- \mathrm{e}^{-2\bar{\theta}_{0:i}}} \mathrm{e}^{-\bar{\theta}_{0:(i-1)}} (\mathbf{x}_{0}-\boldsymbol{\mu}) + \boldsymbol{\mu} \end{equation}\] <p>Given that the second-order derivative with respect to $\mathbf{x}_{i-1}$ is positive:</p> \[\begin{equation} \frac{\mathrm{e}^{-2\bar{\theta}_{(i-1):i}}}{\lambda^2 \left( 1- \mathrm{e}^{-2\bar{\theta}_{(i-1):i}} \right)} + \frac{1}{\lambda^2 \left( 1- \mathrm{e}^{-2\bar{\theta}_{0:(i-1)}} \right)} &gt; 0 \end{equation}\] <p>The solution $\mathbf{x}_{i-1}^*$ is a global optimal.</p> <p><em>—–The End—–</em></p>]]></content><author><name>Bo Peng</name></author><category term="SDE"/><category term="diffusion"/><category term="image-restoration"/><summary type="html"><![CDATA[Theoretical Foundations of SDE-Based Diffusion Models for Image Restoration]]></summary></entry></feed>