<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://ibopeng.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ibopeng.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-25T06:44:04+00:00</updated><id>https://ibopeng.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Tutorial - A Diffusion Model for Image Restoration</title><link href="https://ibopeng.github.io/blog/2024/irsde/" rel="alternate" type="text/html" title="Tutorial - A Diffusion Model for Image Restoration"/><published>2024-11-11T00:00:00+00:00</published><updated>2024-11-11T00:00:00+00:00</updated><id>https://ibopeng.github.io/blog/2024/irsde</id><content type="html" xml:base="https://ibopeng.github.io/blog/2024/irsde/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>In this post, we talk about generative models for image restoration, taking <strong>Image Restoration with Mean-Reverting Stochastic Differential Equations (IR-SDE)</strong> <d-cite key="luo2023image"></d-cite> as an example, a score-based diffusion model for various low-level vision tasks including denoising, super resolution, deraining, etc.</p> <h2 id="background-of-score-based-diffusion-models-through-sdes">Background of Score-based Diffusion Models through SDEs</h2> <p>First, let’s briefly review the basic framework of score-based diffusion models. Let $\mathbf{x}_0 \sim p_0(\mathbf{x})$ denote a sample $\mathbf{x}_0 \in \mathbb{R}^d$ drawn from the initial ground truth data distribution $p_0(\mathbf{x})$. After adding a small amount of noise to $\mathbf{x}_0$, we have a slightly noise-perturbed sample $\mathbf{x}_1 \in \mathbb{R}^d$. By repeating this step for $N$ (e.g., $N$ could be thousands) steps, we obtain a sequence of noisy samples $\{ \mathbf{x}_i \}_{i=0}^N \in \mathbb{R}^d$ indexed by the step variable $i$.</p> <p>Generalizing the discrete step variable $i$ to a continuous time variable $t$, we have a continuous forward diffusion process $\{ \mathbf{x}(t) \}_{t=0}^T \in \mathbb{R}^d$. With sufficiently large $T$, $\mathbf{x}(T)$ is completely noisy without any information about the initial data $\mathbf{x}(0)$.</p> <p><img src="/assets/img/posts/diffusion_forward.png" alt="Forward Diffusion Process" style="width:100%; height:auto; display:block; margin-left:auto; margin-right:auto;"/></p> <p>Note that the initial data distribution $p_0(\mathbf{x})$ is often intractable. However, given a well-defined forward diffusion process, the final noisy sample distribution $\mathbf{x}(T) \sim p_T(\mathbf{x})$ can be in a tractable form (e.g., a standard Gaussian distribution $\mathcal{N}(\mathbf{0}, \mathbf{I})$).</p> <p>To generate samples from such an intractable data distribution $p_0(\mathbf{x})$, a potential way is to reverse the forward diffusion process such that we sample $\mathbf{x}(T)$ from a tractable prior distribution $p_T(\mathbf{x})$, then generate intermediate noisy data $\mathbf{x}(t) \sim p_t(\mathbf{x})$, and finally the clean data $\mathbf{x}(0) \sim p_0(\mathbf{x})$ through a tractable reverse diffusion process running backwards in time, $T \to 0$.</p> <p>In order to achieve this goal, we often start with a tractable <em>prior distribution</em> $p_T(\mathbf{x})$ (e.g., a standard Gaussian distribution $\mathcal{N}(\mathbf{0}, \mathbf{I})$). The forward diffusion process is modeled by a stochastic differential equation (SDE) to perturbe data such that the final output will end up being completely noisy and follow the prior distribution $p_T(\mathbf{x})$.</p> <p>Formally, the SDE for the forward diffusion process can be constructed in the form of:</p> <p>\begin{equation} \label{eq:forward} \mathrm{d} \mathbf{x} = \mathbf{f}(\mathbf{x}, t) \mathrm{d} t + g(t)\mathrm{d} \mathbf{w} \end{equation}</p> <p>where $\mathbf{w}$ is the standard Wiener process (a.k.a., Brownian motion) (time flow: $0 \to T$), $\mathrm{d}t$ is an infinitesimal positive time interval, and $f(\mathbf{x}, t)$ and $g(t)$ are called the <em>drift</em> and <em>diffusion</em> coefficients of $\mathbf{x}$, respectively. Solving the above forward SDE, the intermediate noise-perturbed sample $\mathbf{x}(t)$ can be obtained given the initial data sample $\mathbf{x}(0)$.</p> <p>Note that, the actual SDE can be in different forms such that the prior distribution $p_T(\mathbf{x})$ will have different forms, rather than just limited to standard Gaussiance distribution $\mathcal{N}(\mathbf{0}, \mathbf{I})$. However, we do not have to solve the forward SDE as our goal is to reverse the forward diffusion process or the SDE for generating data $\mathbf{x}(0)$ starting from the noise/prior $\mathbf{x}(T) \sim p_T(\mathbf{x})$:</p> <p><img src="/assets/img/posts/diffusion_backward.png" alt="Backward Diffusion Process" style="width:100%; height:auto; display:block; margin-left:auto; margin-right:auto;"/></p> <p>An important property of the diffusion process modeled by the above SDE (Eq. \eqref{eq:forward}): <em>its reverse process running backwards in time is also a diffusion process modeled by a reverse-time SDE corresponding to the forward SDE</em>.</p> <p>\begin{equation} \label{eq:reverse} \mathrm{d}\mathbf{x} = [ \mathbf{f}(\mathbf{x}, t) - g(t)^2 \underbrace{ \nabla_{\mathbf{x}}\log{p_t(\mathbf{x})} }_{\text{score: } s_{\theta}(\mathbf{x}, t)} ] \mathrm{d}t + g(t)\mathrm{d}\mathbf{\bar{w}} \end{equation}</p> <p>where $\mathbf{\bar{w}}$ is a standard reverse-time Wiener process (time flow: $T \to 0$), $\mathrm{d}t$ is an infinitesimal negative time interval, and $s_{\theta}(\mathbf{x}, t)=\nabla_{\mathbf{x}}\log{p_t(\mathbf{x})}$ is called the score of the marginal distribution $p_t(\mathbf{x})$ for $\mathbf{x}(t)$ at time $t$. Note that the score function $s_{\theta}(\mathbf{x}, t)$ is often intractable and needs to be approximated by a neural network learned from data.</p> <h2 id="image-restoration-with-mean-reverting-sde">Image Restoration with Mean-reverting SDE</h2> <p>In diffusion models, given a sample from the prior distribution, $\mathbf{x}(T) \sim p_T(\mathbf{x})$, we can generate data from the data distribution, $\mathbf{x}(0) \sim p_0(\mathbf{x})$, through the reverse-time SDE. In the problem of image restoration, given the low-quality (LQ) data (e.g., low-resolution images), the goal is to generate high-quality (HQ) data (e.g., high-resolution images). Corresponding to the framework of the SDE-based diffusion processes, we have:</p> <ul> <li>LQ: <em>known</em> prior distribution, $\mathbf{x}(T) \sim p_T(\mathbf{x})$,</li> <li>HQ: <em>unknown</em> data distribution, $\mathbf{x}(0) \sim p_0(\mathbf{x})$</li> </ul> <h3 id="mean-reverting-sde">Mean-reverting SDE</h3> <p>To adapt to the SDE-based diffusion model framework, we need to design a SDE that perturbes the HQ sample $\mathbf{x}(0)$ such that the final noisy counterpart $\mathbf{x}(T)$ follow the same distribution of LQ data.</p> <p>A special form of the forward SDE <d-cite key="luo2023image"></d-cite> was hand designed to satisfy this prerequisite. Let</p> \[\begin{equation} \begin{aligned} \mathbf{f}(\mathbf{x}, t) &amp;= \theta_t(\boldsymbol{\mu} - \mathbf{x}) \\ g(t) &amp;= \sigma_t \end{aligned} \end{equation}\] <p>We have a special SDE:</p> \[\begin{equation} \label{eq:irsde} \mathrm{d}\mathbf{x} = \theta_t(\boldsymbol{\mu} - \mathbf{x}) \mathrm{d}t + \sigma_t \mathrm{d} \mathbf{w} \end{equation}\] <p>where \(\mu\) is the LQ sample, $\theta_t$ and $\sigma_t$ are time-dependent hyperparameters. To sovle the SDE (Eq. \eqref{eq:irsde}), let’s first define a function</p> \[\begin{equation} \phi(\mathbf{x}, t) = \mathbf{x} \mathrm{e}^{\bar{\theta}_t} \end{equation}\] <p>where $\mathbf{x}(t) \sim p_t(\mathbf{x})$ is the data sample at time $t$ and $\bar{\theta}_t=\int_0^t \theta_z \mathrm{d}z$. We can get the differential equation of $\phi(\mathbf{x}, t)$:</p> \[\begin{equation} \mathrm{d} \phi(\mathbf{x}, t) = \mathbf{x} \mathrm{e}^{\bar{\theta}_t} \frac{\partial \bar{\theta}_t}{\partial t} \mathrm{d}t + \mathrm{e}^{\bar{\theta}_t} \mathrm{d} \mathbf{x} \end{equation}\] <p>Note that $\bar{\theta}_t=\int_0^t \theta_z \mathrm{d}z$, $\frac{\partial \bar{\theta}_t}{\partial t} = \theta_t$. Also substituting $\mathrm{d} \mathbf{x}$ with Eq. \eqref{eq:irsde}, we have</p> \[\begin{equation} \begin{aligned} \mathrm{d} \phi(\mathbf{x}, t) &amp;= \mathbf{x} \mathrm{e}^{\bar{\theta}_t} \frac{\partial \bar{\theta}_t}{\partial t} \mathrm{d}t + \mathrm{e}^{\bar{\theta}_t} \mathrm{d} \mathbf{x} \\ &amp;= \mathbf{x} \mathrm{e}^{\bar{\theta}_t} \theta_t \mathrm{d}t + \mathrm{e}^{\bar{\theta}_t} [ \theta_t(\boldsymbol{\mu} - \mathbf{x}) \mathrm{d}t + \sigma_t \mathrm{d} \mathbf{w}_t] \\ &amp;= [\cancel{\mathbf{x} \theta_t \mathrm{e}^{\bar{\theta}_t}} + \boldsymbol{\mu} \theta_t \mathrm{e}^{\bar{\theta}_t} - \cancel{\mathbf{x} \theta_t \mathrm{e}^{\bar{\theta}_t}}] \mathrm{d}t + \sigma_t \mathrm{e}^{\bar{\theta}_t} \mathrm{d} \mathbf{w}_t \\ &amp;= \boldsymbol{\mu} \theta_t \mathrm{e}^{\bar{\theta}_t} \mathrm{d}t + \sigma_t \mathrm{e}^{\bar{\theta}_t} \mathrm{d} \mathbf{w}_t \end{aligned} \end{equation}\] <p>where we change $\mathbf{w}$ to $\mathbf{w}_t$ to explicitly express its change over time $t$. As such, we have the relationship between $\phi(\mathbf{x}, t)$ and $\phi(\mathbf{x}, s)$ at any two different times $s &lt; t$:</p> \[\begin{equation} \begin{aligned} \phi(\mathbf{x}, t) - \phi(\mathbf{x}, s) &amp;= \int_s^t \mathrm{d} \phi(\mathbf{x}, z) \\ &amp;= \int_s^t (\boldsymbol{\mu} \theta_z \mathrm{e}^{\bar{\theta}_z} \mathrm{d}z + \sigma_z \mathrm{e}^{\bar{\theta}_z} \mathrm{d} \mathbf{w}_z) \\ &amp;= \int_s^t \boldsymbol{\mu} \theta_z \mathrm{e}^{\bar{\theta}_z} \mathrm{d}z + \int_s^t \sigma_z \mathrm{e}^{\bar{\theta}_z} \mathrm{d} \mathbf{w}_z \\ \end{aligned} \end{equation}\] <p>Substitute $\phi(\mathbf{x}, t)$ with $\mathbf{x} \mathrm{e}^{\bar{\theta}_t}$ and divide both sides of the above equation by $\mathrm{e}^{\bar{\theta}_t}$, we have</p> \[\begin{equation} \label{eq:xts} \begin{aligned} \mathbf{x}(t) \mathrm{e}^{\bar{\theta}_t} - \mathbf{x}(s) \mathrm{e}^{\bar{\theta}_s} &amp;= \int_s^t \boldsymbol{\mu} \theta_z \mathrm{e}^{\bar{\theta}_z} \mathrm{d}z + \int_s^t \sigma_z \mathrm{e}^{\bar{\theta}_z} \mathrm{d} \mathbf{w}_z \\ \Longrightarrow \mathbf{x}(t) - \mathbf{x}(s) \mathrm{e}^{\bar{\theta}_s - \bar{\theta}_t} &amp;= \int_s^t \boldsymbol{\mu} \theta_z \mathrm{e}^{\bar{\theta}_z - \bar{\theta}_t} \mathrm{d}z + \int_s^t \sigma_z \mathrm{e}^{\bar{\theta}_z - \bar{\theta}_t} \mathrm{d} \mathbf{w}_z \end{aligned} \end{equation}\] <p>Recall that $\bar{\theta}_t=\int_0^t \theta_z \mathrm{d}z$. Given $0 &lt; s &lt; t$, we have</p> \[\begin{equation} \begin{aligned} \mathrm{e}^{\bar{\theta}_s - \bar{\theta}_t} &amp;= \mathrm{e}^{\int_0^s \theta_z \mathrm{d}z - \int_0^t \theta_z \mathrm{d}z} \\ &amp;= \mathrm{e}^{-\int_s^t \theta_z \mathrm{d}z}\\ &amp; = \mathrm{e}^{-\bar{\theta}_{s:t}} \end{aligned} \end{equation}\] <p>Rewriting the above Eq. \eqref{eq:xts}, we obtain</p> \[\begin{equation} \begin{aligned} \mathbf{x}(t) - \mathbf{x}(s) \mathrm{e}^{-\bar{\theta}_{s:t}} &amp;= \int_s^t \boldsymbol{\mu} \theta_z \mathrm{e}^{-\bar{\theta}_{z:t}} \mathrm{d}z + \int_s^t \sigma_z \mathrm{e}^{-\bar{\theta}_{z:t}} \mathrm{d} \mathbf{w}_z \end{aligned} \end{equation}\] <p>Note that $\mathrm{d}(-\bar{\theta}_{z:t}) = \mathrm{d} (-\int_z^t \theta_z \mathrm{d}z) = \theta_z$</p> \[\begin{equation} \int_s^t \boldsymbol{\mu} \theta_z \mathrm{e}^{-\bar{\theta}_{z:t}} \mathrm{d}z = \boldsymbol{\mu} \mathrm{e}^{-\bar{\theta}_{z:t}} \bigg|_s^t = \boldsymbol{\mu} (\mathrm{e}^{-\bar{\theta}_{t:t}} - \mathrm{e}^{-\bar{\theta}_{s:t}}) = \boldsymbol{\mu} (1-\mathrm{e}^{-\bar{\theta}_{s:t}}) \end{equation}\] \[\begin{equation} \label{eq:solution_irsde} \begin{aligned} \mathbf{x}(t) &amp;= \mathbf{x}(s) \mathrm{e}^{-\bar{\theta}_{s:t}} + \boldsymbol{\mu} (1-\mathrm{e}^{-\bar{\theta}_{s:t}}) + \int_s^t \sigma_z \mathrm{e}^{-\bar{\theta}_{z:t}} \mathrm{d} \mathbf{w}_z \\ \mathbf{x}(t) &amp;= \boldsymbol{\mu} + (\mathbf{x}(s) - \boldsymbol{\mu}) \mathrm{e}^{-\bar{\theta}_{s:t}} + \int_s^t \sigma_z \mathrm{e}^{-\bar{\theta}_{z:t}} \mathrm{d} \mathbf{w}_z \end{aligned} \end{equation}\] <p>which is the solution to the SDE in Eq. \eqref{eq:irsde}. For any starting sample $\mathbf{x}_s$ at time $s$, we can generate the noise-perturbed sample $\mathbf{x}_t$ at time $t$. Looking into the expectation of the Eq. \eqref{eq:solution_irsde}, we have</p> \[\begin{equation} \begin{aligned} \mathbb{E}[\mathbf{x}(t)] &amp;= \mathbb{E}\left[\boldsymbol{\mu} + (\mathbf{x}(s) - \boldsymbol{\mu}) \mathrm{e}^{-\bar{\theta}_{s:t}} + \int_s^t \sigma_z \mathrm{e}^{-\bar{\theta}_{z:t}} \mathrm{d} \mathbf{w}_z \right] \\ &amp;= \mathbb{E}\left[\boldsymbol{\mu} + (\mathbf{x}(s) - \boldsymbol{\mu}) \mathrm{e}^{-\bar{\theta}_{s:t}} \right] + \mathbb{E}\left[\int_s^t \sigma_z \mathrm{e}^{-\bar{\theta}_{z:t}} \mathrm{d} \mathbf{w}_z \right] \\ \end{aligned} \end{equation}\] <p>Since the first part $\mu + (\mathbf{x}(s) - \mu) \mathrm{e}^{-\bar{\theta}_{s:t}}$ is deterministic, its expectation is itself</p> \[\begin{equation} \mathbb{E}\left[\boldsymbol{\mu} + (\mathbf{x}(s) - \boldsymbol{\mu}) \mathrm{e}^{-\bar{\theta}_{s:t}} \right] = \boldsymbol{\mu} + (\mathbf{x}(s) - \boldsymbol{\mu}) \mathrm{e}^{-\bar{\theta}_{s:t}} \end{equation}\] <p>Regarding the second part $\mathbb{E}\left[\int_s^t \sigma_z \mathrm{e}^{-\bar{\theta}_{z:t}} \mathrm{d} \mathbf{w}_z \right]$, let’s first review some Itô basics.</p> <hr/> <p><strong>Theorem</strong>. Let $\mathbf{w}_t$ be a Wiener process, and $\Delta(t)$ be a nonrandom function of time. Suppose that a stochastic process $\mathbf{h}(t)$ satisfies</p> \[\begin{equation} \mathrm{d} \mathbf{h} = \Delta(z)\mathrm{d}\mathbf{w}_z \\ \mathbf{h}(t) = \int \Delta(z) \mathrm{d}\mathbf{w}_z \end{equation}\] <p>where $\mathbf{h}(0)=0$. Then, for each $t&gt;=0$, the random variable $\mathbf{h}(t)$ is normally distributed.</p> <hr/> <p>Let $\Delta(z) = \sigma_z \mathrm{e}^{-\bar{\theta}_{z:t}}$ which is nonrandom, $\mathbf{h}(t) = \int_s^t \Delta(z) \mathrm{d} \mathbf{w}_z$, we have</p> \[\begin{equation} \mathbb{E}\left[\int_s^t \sigma_z \mathrm{e}^{-\bar{\theta}_{z:t}} \mathrm{d} \mathbf{w}_z \right] = \mathbb{E}\left[\int_s^t \Delta(z) \mathrm{d} \mathbf{w}_z \right] = \mathbb{E}[\mathbf{h}(t)] = \mathbf{0} \end{equation}\] <p>We can further derive the covariance of $I(t)$.</p> \[\begin{equation} \begin{aligned} \mathrm{Cov}\left(\mathbf{h}(t)\right) &amp;= \mathbb{E} \left[ \left(\mathbf{h}(t)-\mathbb{E}[\mathbf{h}(t)] \right) \left(\mathbf{h}(t)-\mathbb{E}[\mathbf{h}(t)] \right)^\top \right] \\ &amp;= \mathbb{E} \left[ \mathbf{h}(t) \mathbf{h}(t)^\top \right]\\ &amp;= \mathbb{E}\left[ \left(\int_s^t \Delta(z) \mathrm{d} \mathbf{w}_z\right) \left(\int_s^t \Delta(z) \mathrm{d} \mathbf{w}_z\right)^\top \right]\\ \end{aligned} \end{equation}\] <p>Since $\mathbf{h}(t) = \int_s^t \Delta(z) \, \mathrm{d} \mathbf{w}_z$, we can apply the Itô isometry to compute the covariance matrix. For any two components $\mathbf{h}^{(i)}(t)$ and $\mathbf{h}^{(j)}(t)$ of $\mathbf{h}(t)$, we have:</p> \[\begin{equation} \mathbb{E}\left[ \mathbf{h}^{(i)}(t) \mathbf{h}^{(j)}(t) \right] = \mathbb{E}\left[ \int_s^t \Delta(z) \, \mathrm{d} \mathbf{w}_z^{(i)} \int_s^t \Delta(z) \, \mathrm{d} \mathbf{w}_z^{(j)} \right]. \end{equation}\] <p>Since the Wiener processes $\mathbf{w}_z^{(i)}$ and $\mathbf{w}_z^{(j)}$ are independent for $i \neq j$, the cross terms $\mathbb{E}\left[ \mathbf{h}^{(i)}(t) \mathbf{h}^{(j)}(t) \right] = 0$ when $i \neq j$. For the diagonal terms where $i = j$, the Itô isometry gives:</p> \[\begin{equation} \mathbb{E}\left[ \left( \mathbf{h}^{(i)}(t) \right)^2 \right] = \mathbb{E}\left[ \left( \int_s^t \Delta(z) \, \mathrm{d} \mathbf{w}_z^{(i)} \right)^2 \right] = \int_s^t \Delta(z)^2 \, \mathrm{d}z. \end{equation}\] <p>Thus, the covariance matrix $\text{Cov}(\mathbf{h}(t))$ is a diagonal matrix, where each diagonal entry is $\int_s^t \Delta(z)^2 \, \mathrm{d}z$.</p> \[\begin{equation} \begin{aligned} \mathrm{Cov}\left(\mathbf{h}(t)\right) &amp;= \mathbb{E}\left[\left(\int_s^t \Delta(z)^2 \mathrm{d}z\right)\right] \cdot \mathbf{I} \quad (\mathbf{I} \text{ is identity matrix})\\ &amp;= \mathbb{E}\left[\left(\int_s^t \left(\sigma_z \mathrm{e}^{-\bar{\theta}_{z:t}}\right)^2 \mathrm{d}z\right)\right] \cdot \mathbf{I}\\ &amp;= \mathbb{E}\left[\left(\int_s^t \sigma_z^2 \mathrm{e}^{-2\bar{\theta}_{z:t}} \mathrm{d}z\right)\right] \cdot \mathbf{I}\\ &amp;= \mathbb{E}\left[ \frac{\sigma_z^2}{2\theta_z} \mathrm{e}^{-2\bar{\theta}_{z:t}} \bigg|_s^t \right] \cdot \mathbf{I}\\ &amp;= \mathbb{E}\left[ \frac{\sigma_t^2}{2\theta_t} \mathrm{e}^{-2\bar{\theta}_{t:t}} - \frac{\sigma_s^2}{2\theta_s} \mathrm{e}^{-2\bar{\theta}_{s:t}} \right] \cdot \mathbf{I}\\ &amp;= \mathbb{E}\left[ \frac{\sigma_t^2}{2\theta_t} \mathrm{e}^0 - \frac{\sigma_s^2}{2\theta_s} \mathrm{e}^{-2\bar{\theta}_{s:t}} \right] \cdot \mathbf{I}\\ &amp;= \mathbb{E}\left[ \lambda^2 - \lambda^2 \mathrm{e}^{-2\bar{\theta}_{s:t}} \right] \cdot \mathbf{I} \quad (\sigma_t^2 / \theta_t = 2\lambda^2, 0 &lt; t &lt; T) \\ &amp;= \lambda^2 \left( 1- \mathrm{e}^{-2\bar{\theta}_{s:t}} \right) \cdot \mathbf{I} \end{aligned} \end{equation}\] <p>Therefore, we have</p> \[\begin{equation} \mathbf{h}(t) \sim \mathcal{N} \left( \mathbf{0}, \lambda^2 \left( 1- \mathrm{e}^{-2\bar{\theta}_{s:t}} \right) \cdot \mathbf{I} \right) \end{equation}\] <p>Note that $\mathbf{x}(t) \sim p_t(\mathbf{x})$ also follows Gaussian distribution:</p> \[\begin{equation} \begin{aligned} \mathbf{x}(t) &amp;= \boldsymbol{\mu} + \mathrm{e}^{-\bar{\theta}_{s:t}} \cdot \left[ \mathbf{x}(s) - \boldsymbol{\mu} \right] + \int_s^t \sigma_z \mathrm{e}^{-\bar{\theta}_{z:t}} \mathrm{d} \mathbf{w}_z \\ &amp;= \boldsymbol{\mu} + \mathrm{e}^{-\bar{\theta}_{s:t}} \cdot \left[\mathbf{x}(s) - \boldsymbol{\mu} \right] + \mathbf{h}(t) \\ \end{aligned} \end{equation}\] \[\begin{equation} \label{eq:mu_std} \begin{aligned} p(\mathbf{x}(t) | \mathbf{x}(s)) &amp;= \mathcal{N} \left( \mathbf{x}(t) | \boldsymbol{\mu}_{s:t}(\mathbf{x}(s)), \mathbf{\Sigma}_{s:t} \right) \\ \boldsymbol{\mu}_{s:t}(\mathbf{x}(s)) &amp;= \boldsymbol{\mu} + \mathrm{e}^{-\bar{\theta}_{s:t}} \cdot \left[ \mathbf{x}(s) - \boldsymbol{\mu} \right] \\ \mathbf{\Sigma}_{s:t} &amp;= \text{Cov}(\mathbf{h}(t)) = \lambda^2 \left( 1- \mathrm{e}^{-2\bar{\theta}_{s:t}} \right)\cdot \mathbf{I} \end{aligned} \end{equation}\] <p>We observe that, with $t \to \infty$, the mean of $\mathbf{x}_t$, $\mu_{s:t}(\mathbf{x}_s)$ converges to the LQ sample $\mu$ and the covariance $\Sigma_{s:t}$ converges to $\lambda^2$, which is called the <em>stationary variance</em> in the IR-SDE paper <d-cite key="luo2023image"></d-cite>. Therefore, if starting with the initial HQ sample $\mathbf{x}_s = \mathbf{x}_0$, the hand-designed SDE in Eq. \eqref{eq:irsde} established a framework of an SDE-based diffusion model for image restoration, diffusing $\mathbf{x}_0$ to its noisy counterpart, the LQ sample $\mathbf{x}_T$.</p> <h3 id="reversing-the-sde">Reversing the SDE</h3> <p>By Eq. \eqref{eq:reverse}, the reverse-time SDE corresponding to the SDE (Eq. \eqref{eq:irsde}) is:</p> \[\begin{equation} \label{eq:reverse_irsde_custom} \begin{aligned} \mathrm{d}\mathbf{x} &amp;= [\mathbf{f}(\mathbf{x}, t) - g(t)^2 \nabla_{\mathbf{x}}\log{p_t(\mathbf{x})}]\mathrm{d}t + g(t)\mathrm{d}\mathbf{\bar{w}} \\ &amp;= \left[ \theta_t (\boldsymbol{\mu} - \mathbf{x}) - \sigma_t^2 \nabla_{\mathbf{x}} \log p_t(\mathbf{x}) \right] \mathrm{d}t + \sigma_t \mathrm{d} \bar{\mathbf{w}} \end{aligned} \end{equation}\] <p>With this reverse-time SDE flowing backwards in time $T \to 0$, we can starting from the LQ sample $\mathbf{x}(T) \sim p_T(\mathbf{x})$ and generate the HQ sample $\mathbf{x}(0) \sim p_0(\mathbf{x})$. However, we still have an unknown part, the score $\nabla_{\mathbf{x}} \log p_t(\mathbf{x})$, in the reverse-time SDE.</p> <p>As the score function $\nabla_{\mathbf{x}} \log p_t(\mathbf{x})$ is intractable for real-world data, it is often approximated by a neural network trained with ground-truth HQ-LQ sample pairs, $\left(\mathbf{x}(0), \mathbf{x}(T)\right)$. To train a neural network mode $s_{\theta}(\mathbf{x}(t), t)$ given time $t$ and the intermediate noisy sample $\mathbf{x}(t)$, we first compute the conditional ground-truth score $\nabla_{\mathbf{x}} \log p_t(\mathbf{x} | \mathbf{x}(0))$ given the HQ sample $\mathbf{x}(0)$. Note that</p> \[\begin{equation} \label{eq:prob_s_t} p(\mathbf{x}(t) | \mathbf{x}(s)) = \mathcal{N} \left( \mathbf{x}(t) | \boldsymbol{\mu}_{s:t}(\mathbf{x}(s)), \mathbf{\Sigma}_{s:t} \right) \end{equation}\] <p>we have</p> \[\begin{equation} p_t(\mathbf{x} | \mathbf{x}(0)) = \mathcal{N} \left( \mathbf{x}(t) | \boldsymbol{\mu}_{0:t}(\mathbf{x}(0)), \mathbf{\Sigma}_{0:t} \right) \end{equation}\] <p>To simplify notation, we replace all subscripts $\{ * \}_{0:t}$ with $\{ * \}_{t}$. Thus, we have</p> \[\begin{equation} \begin{aligned} p_t(\mathbf{x} | \mathbf{x}(0)) &amp;= \mathcal{N} \left( \mathbf{x}(t) | \boldsymbol{\mu}_{t}, \mathbf{\Sigma}_{t} \right) \\ &amp;= \frac{1}{(2 \pi)^{d/2} |\mathbf{\Sigma}_{t}|^{1/2}} \exp\left( -\frac{1}{2} \left(\mathbf{x}(t) - \boldsymbol{\mu}_{t}\right)^\top \mathbf{\Sigma}_{t}^{-1} \left(\mathbf{x}(t) - \boldsymbol{\mu}_{t}\right) \right) \end{aligned} \end{equation}\] <p>Hence, the score has the following closed-form solution:</p> \[\begin{equation} \nabla_{\mathbf{x}} \log p_t(\mathbf{x} | \mathbf{x}(0)) = -\mathbf{\Sigma}_{t}^{-1} \left( \mathbf{x}(t) - \boldsymbol{\mu}_{t} \right) \end{equation}\] <h3 id="training-objectives">Training Objectives</h3> <h4 id="noise-prediction">Noise Prediction</h4> <p>With ground-truth clean sample $\mathbf{x}(0)$ in training, we first perturbe $\mathbf{x}(0)$ through the diffusion process via the forward SDE and obtain the noise-perturbed sample $\mathbf{x}(t)$.</p> <p>Next, we compute the mean $\mu_{t}$ and covariance $\Sigma_{t}$ by Eq. \eqref{eq:mu_std} such that we can obtain the ground-truth score $\nabla_{\mathbf{x}}\log{p_t(\mathbf{x}|\mathbf{x}(0))}$ for training the score network $\mathbf{s}_{\theta}(\mathbf{x}(t),t)$.</p> <p>During inference, the reverse-time SDE starts from the final time $T$ such that the score network approximates the score at time $T$ given the most noisy input LQ sample, $\mathbf{x}(T)$. Ideally, after iteratively running backwards in time, $T \to 0$, the reverse process will generate the clean HQ sample, $\mathbf{x}(0)$, via the reverse-time SDE (Eq. \eqref{eq:reverse_irsde_custom}). However, the actual performance might not be as expected, as detailed in many studies.</p> <p>Considering the conditional distribution, $p_t(\mathbf{x} | \mathbf{x}(0)) \sim \mathcal{N}(\mu_t, \Sigma_t)$, we have $\mathbf{x}(t) = \mu_{t} + |\Sigma_t|^{1/2} \epsilon_t$, where $\epsilon_t \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$. The score is reparameterized as</p> \[\begin{equation} \nabla_{\mathbf{x}} \log p_t(\mathbf{x} | \mathbf{x}(0)) = -\mathbf{\Sigma}_{t}^{-1/2} \boldsymbol{\epsilon}_t \end{equation}\] <p>Therefore, we can, instead, train a noise network $\tilde{\epsilon}_{\phi}(\mathbf{x}(t), \mu, t)$ which takes the intermediate noisy sample $\mathbf{x}(t)$, the LQ sample or condition $\mu$, and the time $t$ as input and outputs the noise to approximate the ground-truth noise $\epsilon_t$.</p> <p>The noise network can be trained following a similar framework proposed in the work Denoising Diffusion Probabilistic Models (DDPM) <d-cite key="ho2020denoising"></d-cite> by minimizing the following loss (after discretization from continuous time variable $t \in [0, T]$ to discretized step variable $\{ i\}_{i=0}^N$):</p> \[\begin{equation} L_{\gamma}(\phi) = \sum_{i=1}^{N}\gamma_i \mathbb{E} \left[ \Vert \tilde{\boldsymbol{\epsilon}}_{\phi}(\mathbf{x}_i, \boldsymbol{\mu}, i) - \boldsymbol{\epsilon}_i \Vert \right] \end{equation}\] <p>where, for the $i^{\text{th}}$ step in the diffusion process, $\gamma_i$ denotes the weight, $\mathbf{x}_i$ represents the intermediate noisy sample, and ${\epsilon}_i$ is the ground-truth randomly sampled Gaussian noise.</p> <p>After the noise network $\tilde{\epsilon}_{\phi}(\mathbf{x}_i, {\mu}, i)$ is trained, we can start from the LQ sample $\mathbf{x}_N$ and iteratively solve the reverse-time SDE with numerical approaches (e.g., Euler-Maruyama or stochastic Runge-Kutta methods) for generating the HQ sample $\mathbf{x}_0$. More details can be found in the study, Score-based Diffusion Models through SDE <d-cite key="song2020score"></d-cite>.</p> <h4 id="maximum-likelihood">Maximum Likelihood</h4> <p>Unfortunately, the above objective $L_{\gamma}(\phi)$ did not produce good HQ samples in experiments while there is not good reason in the original paper explaining this issue. Instead, a new maximum likelihood objective was proposed for training the noise network.</p> <p>In this part, let’s the diffusion process at discrete times $\{i\}_{i=0}^N$ and the Markov property. The joint probability of $(\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_N )$ conditioned on $\mathbf{x}_0$ is</p> \[\begin{equation} p(\mathbf{x}_{1:N}|\mathbf{x}_0) = p(\mathbf{x}_N|\mathbf{x}_0) \prod_{i=2}^N p(\mathbf{x}_{i-1}|\mathbf{x}_{i}, \mathbf{x}_0) \end{equation}\] <p>where we consider the intermediate noisy sample $\mathbf{x}_{i-1}$ is conditioned on both the initial HQ sample $\mathbf{x}_0$ and the next noise-perturbed sample $\mathbf{x}_{i}$.</p> <p>Using Bayes’ rule, the reverse transition, $p(\mathbf{x}_{i-1} | \mathbf{x}_i, \mathbf{x}_0)$ can be expressed as</p> \[\begin{equation} \begin{aligned} p(\mathbf{x}_{i-1}|\mathbf{x}_{i}, \mathbf{x}_0) &amp;= \frac{p(\mathbf{x}_{i}|\mathbf{x}_{i-1}, \mathbf{x}_0) p(\mathbf{x}_{i-1}|\mathbf{x}_0)}{p(\mathbf{x}_i|\mathbf{x}_0)} \\ &amp;= \frac{p(\mathbf{x}_{i}|\mathbf{x}_{i-1}) p(\mathbf{x}_{i-1}|\mathbf{x}_0)}{p(\mathbf{x}_i|\mathbf{x}_0)} \end{aligned} \end{equation}\] <p>where each of the above probabilities are Gaussian (Eq. \eqref{eq:prob_s_t}). The goal is to find the optimal $\mathbf{x}_{i-1}^*$ to maximize the likelihood $p(\mathbf{x}_{i-1} | \mathbf{x}_i, \mathbf{x}_0)$ or minimize the negative log likelihood (NLL) $- \log p(\mathbf{x}_{i-1} | \mathbf{x}_i, \mathbf{x}_0)$.</p> \[\begin{equation} \begin{aligned} \mathbf{x}_{i-1}^* &amp;= \arg \max_{\mathbf{x}_{i-1}} \left[ p(\mathbf{x}_{i-1}|\mathbf{x}_{i}, \mathbf{x}_0)\right] \\ &amp;= \arg \max_{\mathbf{x}_{i-1}} \left[ p(\mathbf{x}_{i}|\mathbf{x}_{i-1}) p(\mathbf{x}_{i-1}|\mathbf{x}_0) \right] \\ &amp;= \arg \min_{\mathbf{x}_{i-1}} \left[ -\log p(\mathbf{x}_{i}|\mathbf{x}_{i-1}) - \log p(\mathbf{x}_{i-1}|\mathbf{x}_0) \right] \\ \end{aligned} \end{equation}\] <p>The discretized version of the transition kernel, $p(\mathbf{x}_t | \mathbf{x}_s)$, between times $s$ and $t$, becomes:</p> \[\begin{equation} \label{eq:prob_s_t_discrete} \begin{aligned} p(\mathbf{x}_t | \mathbf{x}_s) &amp;= \mathcal{N} \left( \mathbf{x}_t | \boldsymbol{\mu}_{s:t}(\mathbf{x}_s), \mathbf{\Sigma}_{s:t} \right) \\ \boldsymbol{\mu}_{s:t}(\mathbf{x}_s) &amp;= \boldsymbol{\mu} + \mathrm{e}^{-\bar{\theta}_{s:t}} \cdot \left[ \mathbf{x}_s - \boldsymbol{\mu} \right] \\ \mathbf{\Sigma}_{s:t} &amp;= \text{Cov}(\mathbf{h}_t) = \lambda^2 \left( 1- \mathrm{e}^{-2\bar{\theta}_{s:t}} \right)\cdot \mathbf{I} \end{aligned} \end{equation}\] <p>As the transitions $p(\mathbf{x}_{i} | \mathbf{x}_{i-1})$ and $p(\mathbf{x}_{i-1} | \mathbf{x}_0)$ have been derived in Eq. \eqref{eq:prob_s_t_discrete}, we can solve the above optimization problem by taking the derivative.</p> \[\begin{equation} \begin{aligned} &amp; - \nabla_{\mathbf{x}_{i-1}} \log p(\mathbf{x}_{i}|\mathbf{x}_{i-1}) - \nabla_{\mathbf{x}_{i-1}} \log p(\mathbf{x}_{i-1}|\mathbf{x}_0) \\ = &amp; - \mathbf{\Sigma}_{(i-1):i}^{-1} (\mathbf{x}_{i} - \boldsymbol{\mu}_{(i-1):i})\mathrm{e}^{-\bar{\theta}_{(i-1):i}} - \mathbf{\Sigma}_{0:(i-1)}^{-1} (\mathbf{x}_{i-1} - \boldsymbol{\mu}_{0:(i-1)}) \\ = &amp; -\frac{\mathrm{e}^{-\bar{\theta}_{(i-1):i}}}{\lambda^2 \left( 1- \mathrm{e}^{-2\bar{\theta}_{(i-1):i}} \right)} (\mathbf{x}_{i} - \boldsymbol{\mu} - \mathrm{e}^{-\bar{\theta}_{(i-1):i}} \cdot ( \mathbf{x}_{i-1} - \boldsymbol{\mu} )) \\ &amp; +\frac{1}{\lambda^2 \left( 1- \mathrm{e}^{-2\bar{\theta}_{0:(i-1)}} \right)} (\mathbf{x}_{i-1} - \boldsymbol{\mu} - \mathrm{e}^{-\bar{\theta}_{0:(i-1)}} \cdot \left( \mathbf{x}_0 - \boldsymbol{\mu} \right)) \\ = &amp; 0 \end{aligned} \end{equation}\] <p>Setting the above derivative to 0 and solving the equation, we have:</p> \[\begin{equation} \mathbf{x}_{i-1}^* = \frac{1- \mathrm{e}^{-2\bar{\theta}_{0:(i-1)}}}{1- \mathrm{e}^{-2\bar{\theta}_{0:i}}} \mathrm{e}^{-\bar{\theta}_{(i-1):i}} (\mathbf{x}_{i}-\boldsymbol{\mu}) + \frac{1- \mathrm{e}^{-2\bar{\theta}_{(i-1):i}}}{1- \mathrm{e}^{-2\bar{\theta}_{0:i}}} \mathrm{e}^{-\bar{\theta}_{0:(i-1)}} (\mathbf{x}_{0}-\boldsymbol{\mu}) + \boldsymbol{\mu} \end{equation}\] <p>Given that the second-order derivative with respect to $\mathbf{x}_{i-1}$ is positive:</p> \[\begin{equation} \frac{\mathrm{e}^{-2\bar{\theta}_{(i-1):i}}}{\lambda^2 \left( 1- \mathrm{e}^{-2\bar{\theta}_{(i-1):i}} \right)} + \frac{1}{\lambda^2 \left( 1- \mathrm{e}^{-2\bar{\theta}_{0:(i-1)}} \right)} &gt; 0 \end{equation}\] <p>The solution $\mathbf{x}_{i-1}^*$ is a global optimal.</p> <p><em>—–The End—–</em></p>]]></content><author><name>Bo Peng</name></author><category term="SDE"/><category term="diffusion"/><category term="image-restoration"/><summary type="html"><![CDATA[Theoretical Foundations of SDE-Based Diffusion Models for Image Restoration]]></summary></entry></feed>